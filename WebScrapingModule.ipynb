{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from src.classes.WebScraper import WebScraper\n",
    "# from textblob import TextBlob\n",
    "# import numpy as np\n",
    "# \n",
    "# sentiments = []\n",
    "# \n",
    "# scraper = WebScraper()\n",
    "# news = scraper.search_and_scrape('bitcoin 23.02.2022')\n",
    "# \n",
    "# for article in news:\n",
    "#     for paragraph in article:\n",
    "#         blob = TextBlob(paragraph)\n",
    "#         sentiments.append(blob.sentiment.polarity)\n",
    "# \n",
    "# average_sentiment = np.mean(sentiments)\n",
    "# \n",
    "# print(f\"average_sentiment: {average_sentiment}\")\n",
    "# \n",
    "# if average_sentiment > 0:\n",
    "#     print(\"Общий настрой рынка: Позитивный\")\n",
    "# elif average_sentiment < 0:\n",
    "#     print(\"Общий настрой рынка: Негативный\")\n",
    "# else:\n",
    "#     print(\"Общий настрой рынка: Нейтральный\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T19:29:35.955835500Z",
     "start_time": "2024-01-21T19:29:35.891611200Z"
    }
   },
   "id": "df0e430e62c419c5",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "Positive: 0.9758\n",
      "Positive: 0.9984\n",
      "Positive: 0.9325\n",
      "Positive: 0.9975\n",
      "Positive: 0.999\n",
      "Positive: 0.9987\n",
      "Positive: 0.9981\n",
      "Positive: 0.9957\n",
      "Negative: -0.9705\n",
      "Positive: 0.9736\n",
      "Total Positive: 8.8693\n",
      "Total Negative: 0.9705\n",
      "Total Neutral: 0\n"
     ]
    }
   ],
   "source": [
    "from src.classes.WebScraper import WebScraper\n",
    "from statsmodels.tsa.statespace import news\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "scraper = WebScraper()\n",
    "web_scrapper_news = scraper.search_and_scrape('bitcoin 23.02.2022')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# \n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "for text in web_scrapper_news:\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        positive += sentiment['compound']\n",
    "        print(f\"Positive: {sentiment['compound']}\")\n",
    "    elif sentiment['compound'] <= -0.05:\n",
    "        negative -= sentiment['compound']\n",
    "        print(f\"Negative: {sentiment['compound']}\")\n",
    "    else:\n",
    "        neutral += sentiment['compound']\n",
    "        print(f\"Neutral: {sentiment['compound']}\")\n",
    "\n",
    "print(f'Total Positive: {positive}')\n",
    "print(f'Total Negative: {negative}')\n",
    "print(f'Total Neutral: {neutral}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T19:29:32.712468600Z",
     "start_time": "2024-01-21T19:28:53.499701900Z"
    }
   },
   "id": "281f16740985ed90",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "117b344ddb5d4a9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.handlers.NewsHandler import NewsHandler\n",
    "\n",
    "\n",
    "news_handler = NewsHandler(\"BTC\", \"20.03.2022\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T19:45:45.481069200Z",
     "start_time": "2024-01-21T19:45:32.684351200Z"
    }
   },
   "id": "18d321afa9bb4d2a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n",
      "WARNING:tensorflow:From C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoodState.POSITIVE\n"
     ]
    }
   ],
   "source": [
    "result = news_handler.handle()\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T19:48:39.363658200Z",
     "start_time": "2024-01-21T19:45:45.471076500Z"
    }
   },
   "id": "4cebf163547bc8aa",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb119fa61a9856b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T18:27:01.253197900Z",
     "start_time": "2024-01-21T18:27:01.187822500Z"
    }
   },
   "id": "b5aae1028e519f96",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections found:  ['google']\n",
      "Sections found:  ['google']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.services.Config import Config\n",
    "\n",
    "config = Config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T18:27:01.254197Z",
     "start_time": "2024-01-21T18:27:01.195067900Z"
    }
   },
   "id": "f79be872b3d10df5",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T18:27:01.255196400Z",
     "start_time": "2024-01-21T18:27:01.203855600Z"
    }
   },
   "id": "f6809ac14f73ef95",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.services.Logger import Logger\n",
    "\n",
    "Logger().add_log(log=\"TEST\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T18:27:01.256196300Z",
     "start_time": "2024-01-21T18:27:01.211671600Z"
    }
   },
   "id": "d192249bdc5838fe",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
